{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRO_MFFj_iYh"
   },
   "source": [
    "<img src=\"./assets/mellea_banner.png\" width=\"80%\"/>\n",
    "\n",
    "# Mellea workshop\n",
    "\n",
    "[Notebook on github](https://github.com/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb),\n",
    "[Notebook on colab](https://colab.research.google.com/github/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb)\n",
    "\n",
    "This workshop will introduce the basics of Generative Computing through a series of labs following a use-case.\n",
    "\n",
    "During this workshop, we will:\n",
    "1. Get up an running with Mellea.\n",
    "2. See the Instruct - Validate - Repair pattern in action.\n",
    "3. Encapsulate LLM calls using a functional interface via Mellea's `@generative` decorator.\n",
    "4. Combine Mellea and Docling to write a generative program that operates over documents.\n",
    "5. Leverage a vision model to analyse an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "\n",
    "Run the first cell during our introduction. The first cell will:\n",
    " * download an install ollama on your Colab instance\n",
    " * pull the granite4 and granite-vision model weights from ollama\n",
    " * pull the docling model weights\n",
    "\n",
    "This should take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fDEbLHL_hkK"
   },
   "outputs": [],
   "source": [
    "print(\"Install ollama\")\n",
    "# - When running locally, install ollama app from https://ollama.com, and the service should start automatically with mac/windows\n",
    "# - When running on colab uncomment the next two lines to install and start ollama in the colab environment:\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
    "# !nohup ollama serve >/dev/null 2>&1 &\n",
    "\n",
    "print(\"Download model weights\")\n",
    "# Download model weights for the Granite 4 and Granite vision models\n",
    "!ollama pull ibm/granite4:micro\n",
    "!ollama pull granite3.2-vision\n",
    "print(\"Install Mellea with Docling\")\n",
    "!uv pip install mellea docling jupyter ipywidgets \n",
    "print(\"Run docling once to download model weights\")\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")\n",
    "\n",
    "# Some UI niceness\n",
    "# from IPython.display import HTML, display  # noqa: E402\n",
    "\n",
    "# def set_css():\n",
    "#     display(HTML(\"\\n<style>\\n pre{\\n white-space: pre-wrap;\\n}\\n</style>\\n\"))\n",
    "\n",
    "# get_ipython().events.register(\"pre_run_cell\", set_css)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Mellea.ai\n",
    "\n",
    "## *“The second most remarkable thing about LLMs is that they can understand and produce fluent natural language. The most remarkable thing about them is that they do the wrong thing 5-50% of the time”*\n",
    "\n",
    "### > **We need structures** that make it a first-class concern **to check our work**, and ideally, we should build new tools to allow interrogation of certainty.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## What is “generative computing”?\n",
    "\n",
    "*Generative computing* is the idea that **LLMs** can function **as computing elements** that are a part of, not separate from, the rest of computer science.\n",
    "\n",
    "Like computing in general, this will have many facets, spanning engineering practice, system design, theory, hardware, software, etc.\n",
    "\n",
    "Many aspects are already emerging in the field, but they could use a nudge, and **we’re building tools to accelerate progress**.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Mellea.ai\n",
    "\n",
    "### Local-first Capability\n",
    "Generative programs can do Big Model Things without Big Model hardware.\n",
    "\n",
    "### Robust and Composable\n",
    "Requirement-driven inference pipelines result in rock-solid libraries and deployable apps.\n",
    "\n",
    "### Ready to Scale\n",
    "Swap out inference engines and models with ease. Massively scale inference in one line of code.\n",
    "\n",
    "### Open & Permissive\n",
    "The open inference stack for building generative programs on open weight models.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Applied Principles for Mellea.ai\n",
    "\n",
    "5 steps to showcase Mellea features with a unique workshop:\n",
    "\n",
    "### 01.  Easy prompting\n",
    "Automated response via LLM\n",
    "\n",
    "### 02. Validate & Retry\n",
    "Generated response guided-verification\n",
    "\n",
    "### 03. Function-Generation\n",
    "Satisfaction Detection through Stub Generation\n",
    "\n",
    "### 04. Embedded Document processing\n",
    "Invoice simplified Analysis\n",
    "\n",
    "### 05. Vision Model\n",
    "Product identification in a photo\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Use-case overview\n",
    "\n",
    "Let's sell T-Shirts!\n",
    "\n",
    "<iframe src=\"https://mellea-ai-store.figma.site/\" width='100%', height=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpHGZpSt5mi4"
   },
   "source": [
    "# Lab 1: Hello, Mellea!\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "- Run locally with Ollama\n",
    "- Get started with 3 lines of code\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "<TBD>\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "Running `mellea.start_session()` initialize a new `MelleaSession`. The session holds three things:\n",
    "1. The model to use for this session. In this tutorial we will use granite3.3:8b.\n",
    "2. An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.\n",
    "3. A `Context`, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout this tutorial, we will be using a `SimpleContext`. In `SimpleContext`s, **every request starts with a fresh context**. There is no preserved chat history between requests. Mellea provides other types of context, but today we will not be using those features. See the Tutorial for further details.\n",
    "\n",
    "## Let's experiment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlF315Wa_pGh"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "answer = m.chat(\n",
    "    \"tell me some fun trivia about IBM and the early history of AI.\"\n",
    ")\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "import mellea\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "answer = m.chat(\"tell me some fun trivia about IBM and the early history of AI.\")\n",
    "print(answer.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI5ZHPOX7I-7"
   },
   "source": [
    "# Lab 2: Instruct-Validate-Repair\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:\n",
    "1. Instruct the model to perform a task and specify requirements on the output of the task.\n",
    "2. Validate that these requirements are satisfied by the model's output.\n",
    "3. If any requirements fail, try to repair.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "We will prepare an e-mail answer, and check for specific aspects in the generated email.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- Build a requirements list with the `req()` and `check()` constructors.\n",
    "- use the `instruct()` method to ask Mellea to perform an instruction and retry until the requirements are met.\n",
    "\n",
    "## Let's experiment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeKAbcqDBgnk"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.requirement import check, req, simple_validate\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "requirements_list = [\n",
    "    req(\"The email should have a salutation\"),\n",
    "    req(\n",
    "        \"Use only lower-case letters\",\n",
    "        validation_fn=simple_validate(lambda x: x.lower() == x),\n",
    "    ),\n",
    "    check(\"Do not mention purple elephants.\"),\n",
    "]\n",
    "\n",
    "name=\"Olivia\"\n",
    "notes=\"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\"\n",
    "\n",
    "email_candidate = m.instruct(\n",
    "    description=\"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
    "    requirements=requirements_list,\n",
    "    strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "    user_variables={\"name\": name, \"notes\": notes},\n",
    "    return_sampling_results=True,\n",
    ")\n",
    "if email_candidate.success:\n",
    "    print(email_candidate.result)\n",
    "else:\n",
    "    print(email_candidate.sample_generations[0].value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "<TBD>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eljw3pUP8WkP"
   },
   "source": [
    "## Lab 3: Generative Stubs\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "In classical programming, pure (stateless) functions are a simple and powerful abstraction. A pure function takes inputs, computes outputs, and has no side effects. Generative programs can also use functions as abstraction boundaries, but in a generative program the meaning of the function can be given by an LLM instead of an interpreter or compiler. This is the idea behind a GenerativeSlot.\n",
    "\n",
    "A GenerativeSlot is a function whose implementation is provided by an LLM. In Mellea, you define these using the `@generative` decorator. The function signature specifies the interface, and the docstring (or type annotations) guide the LLM in producing the output. Let's start with a simple example of a sentiment classifier using the Generative interface.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Setup a classifier to detect the sentiment of the client's email, and use it to customize our answer.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `@generative` annotation to create a runnable method from a text description\n",
    "- determine possible outputs with the `Literal[]` type annnotation\n",
    "\n",
    "## Let's experiment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "m = mellea.start_session()\n",
    "\n",
    "@generative\n",
    "def sentiment_classifier(text: str) -> None|Literal[\"positive\", \"negative\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive or negative.\"\"\"\n",
    "\n",
    "sentiment = sentiment_classifier(m, text=\"The weather in Orlando is beautiful today!\")\n",
    "print(f\"Detected sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "<TBD>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Docling and Mellea\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "In this lab, we will use both Docling and Mellea to extract data from a PDF.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Use docling to read the invoice attached by the client and check for the invoice id.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `RichDocument.from_document_file` to load a pdf file\n",
    "- explore the document\n",
    "\n",
    "## Let's experiment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVH1fmWB3JHp"
   },
   "outputs": [],
   "source": [
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "<TBD>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Vision models\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "Analyse an image by leveraging a vision model directly in Mellea\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Use a vision model to read a photo provided by the client and validate the item to be returned.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- start a mellea session with a model vision\n",
    "- load an image\n",
    "- pass the image as context to the `instruct()` method\n",
    "\n",
    "## Let's experiment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from PIL import Image\n",
    "\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# load image\n",
    "test_img = Image.open(\"./assets/mellea_pointing_up.jpg\")\n",
    "\n",
    "# ask a question about the image\n",
    "res = m.instruct(\"Is the subject in the image smiling?\", images=[test_img])\n",
    "print(f\"Result:{res!s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "<TBD>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "githubcom-rs-mellea-workshop (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
