{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRO_MFFj_iYh"
   },
   "source": [
    "<img src=\"./assets/mellea_banner.png\" width=\"80%\"/>\n",
    "\n",
    "# Mellea workshop - [ibm.biz/mellea-workshop](https://ibm.biz/mellea-workshop)\n",
    "\n",
    "[Notebook on github](https://github.com/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb),\n",
    "[Notebook on colab](https://colab.research.google.com/github/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb)\n",
    "\n",
    "This workshop will introduce the basics of Generative Computing through a series of labs following a use-case.\n",
    "\n",
    "During this workshop, we will:\n",
    "1. Get up an running with Mellea.\n",
    "2. See the Instruct - Validate - Repair pattern in action.\n",
    "3. Encapsulate LLM calls using a functional interface via Mellea's `@generative` decorator.\n",
    "4. Combine Mellea and Docling to write a generative program that operates over documents.\n",
    "5. Leverage a vision model to analyse an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "\n",
    "Run the first cell during our introduction. The first cell will:\n",
    " * download an install ollama on your Colab instance\n",
    " * pull the granite4 and granite-vision model weights from ollama\n",
    " * pull the docling model weights\n",
    "\n",
    "This should take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fDEbLHL_hkK"
   },
   "outputs": [],
   "source": [
    "print(\"Install and start Ollama\")\n",
    "# - When running locally, install ollama app from https://ollama.com, and the service should start automatically with mac/windows\n",
    "# - When running on colab we download and start ollama in the colab environment\n",
    "import subprocess\n",
    "if subprocess.run([\"which\", \"ollama\"]).returncode == 0:\n",
    "    print(\"Ollama already installed\")\n",
    "else:\n",
    "    print(\"Installing Ollama\")\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
    "\n",
    "if subprocess.run([\"ollama\",\"ps\"]).returncode == 0:\n",
    "    print(\"Ollama already running\")\n",
    "else:\n",
    "    print(\"Starting Ollama\")\n",
    "    !nohup ollama serve >/dev/null 2>&1 &\n",
    "    !sleep 5\n",
    "\n",
    "print(\"Download model weights\")\n",
    "# Download model weights for the Granite 4 and Granite vision models\n",
    "!ollama pull ibm/granite4:micro\n",
    "!ollama pull granite3.2-vision\n",
    "print(\"Install Mellea with Docling\")\n",
    "!uv pip install mellea docling jupyter ipywidgets \n",
    "print(\"Run docling once to download model weights\")\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")\n",
    "# Init asset path\n",
    "asset_path = \"https://raw.githubusercontent.com/remiserra/mellea-workshop/refs/heads/main/assets/\"\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Mellea.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *“The second most remarkable thing about LLMs is that they can understand and produce fluent natural language. The most remarkable thing about them is that they do the wrong thing 5-50% of the time”*\n",
    "\n",
    "### > **We need structures** that make it a first-class concern **to check our work**, and ideally, we should build new tools to allow interrogation of certainty.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## What is “generative computing”?\n",
    "\n",
    "*Generative computing* is the idea that **LLMs** can function **as computing elements** that are a part of, not separate from, the rest of computer science.\n",
    "\n",
    "Like computing in general, this will have many facets, spanning engineering practice, system design, theory, hardware, software, etc.\n",
    "\n",
    "Many aspects are already emerging in the field, but they could use a nudge, and **we’re building tools to accelerate progress**.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Mellea.ai\n",
    "\n",
    "### Local-first Capability\n",
    "Generative programs can do Big Model Things without Big Model hardware.\n",
    "\n",
    "### Robust and Composable\n",
    "Requirement-driven inference pipelines result in rock-solid libraries and deployable apps.\n",
    "\n",
    "### Ready to Scale\n",
    "Swap out inference engines and models with ease. Massively scale inference in one line of code.\n",
    "\n",
    "### Open & Permissive\n",
    "The open inference stack for building generative programs on open weight models.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Applied Principles for Mellea.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 steps to showcase Mellea features with a unique workshop:\n",
    "\n",
    "### 01. Easy prompting\n",
    "Automated response via LLM\n",
    "\n",
    "### 02. Instruct - Validate - Repair\n",
    "Generated response guided-verification\n",
    "\n",
    "### 03. Function-Generation\n",
    "Generative functions through desciptions\n",
    "\n",
    "### 04. Embedded Document processing\n",
    "Easy integration of document processing pipeline\n",
    "\n",
    "### 05. Vision Model\n",
    "Analyse images with vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Use-case overview\n",
    "We are the lucky owners of an apparel store dedicated to the Mellea brand. \n",
    "\n",
    "Let's sell T-Shirts on our [Mellea.ai Store](https://mellea-ai-store.figma.site/)!\n",
    "\n",
    "<center><img src=\"./assets/mellea_store.png\" width=\"80%\" border=1/></center>\n",
    "\n",
    "<!-- <iframe src=\"https://mellea-ai-store.figma.site/\" width=\"100%\" height=400></iframe> -->\n",
    "\n",
    "We've been struggling to handle returns while running the physical shop, and honestly, it's become a burden for everyone. \n",
    "\n",
    "People are mainly using our contact page, and it's a bit overwhelmed. Processing individual requests and maintaining ongoing conversations with each customer is extremely time-consuming.\n",
    "\n",
    "Our goal today is to make returns easier by automating the answers of our customer on our website, thanks to mellea-ai! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpHGZpSt5mi4"
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 1: Hello, Mellea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellea features\n",
    "\n",
    "- Run locally with Ollama\n",
    "- Get started with 3 lines of code\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "We will start by preparing a simple welcome message for our apparel retail website.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "Running `mellea.start_session()` initialize a new `MelleaSession`. The session holds three things:\n",
    "1. The model to use for this session. In this tutorial we will use granite models. Starting with the default (currently granite4 micro)\n",
    "2. An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.\n",
    "3. A `Context`, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout this tutorial, we will be using a `SimpleContext`. In `SimpleContext`s, **every request starts with a fresh context**. There is no preserved chat history between requests. Mellea provides other types of context, but today we will not be using those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlF315Wa_pGh"
   },
   "outputs": [],
   "source": [
    "# Import mellea library\n",
    "import mellea\n",
    "\n",
    "# Start mella session with default model\n",
    "m = mellea.start_session()\n",
    "\n",
    "# Send a message to the model\n",
    "answer = m.chat(\"Please write a welcome message for the Mellea apparel website\")\n",
    "\n",
    "# Print the answer\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 2: Instruct - Validate - Repair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI5ZHPOX7I-7"
   },
   "source": [
    "## Mellea features\n",
    "\n",
    "Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:\n",
    "1. Instruct the model to perform a task and specify requirements on the output of the task.\n",
    "2. Validate that these requirements are satisfied by the model's output.\n",
    "3. If any requirements fail, try to repair.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Let's prepare an email response to a return request and check for specific aspects in the generated email. \n",
    "\n",
    "The system should automatically validate that the email includes a proper salutation, contains our website URL (mellea.ai).\n",
    "\n",
    "It should also check if the response is written in correct English, even if the customer request is in another language. \n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- Build a requirements list with the `req()` and `check()` constructors.\n",
    "- use the `instruct()` method to ask Mellea to perform an instruction and retry until the requirements are met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the client name and request as inputs\n",
    "client_name = \"Joseph\"\n",
    "client_request = \"Hi, I just received my Amanita T-sirt, it's really nice ! Unforunately it is too small for me, how can I return it ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have instructions about the information to include in our emails\n",
    "email_prompt = \"\"\"\n",
    "You are an automated customer service representative for an apparel boutique called Mellea Store. Your task is to generate a formal, polite, and email-ready response to customers who contact the shop regarding a product return. \n",
    "\n",
    "Follow these rules when writing your answer:\n",
    "- Always reply using professional and courteous language suitable for an email. \n",
    "- Address the customer respectfully using “Hello” and close with “Regards” followed by “Mellea Store Team” \n",
    "- Include the standard French return policy terms compliant with consumer law. \n",
    "- The answer is concise. The lenght of the answer should be 120 words maximum.  \n",
    "\n",
    "Your email must contain the following sections: \n",
    "\n",
    "1. Acknowledgement of the request\n",
    "Confirm receipt of the return request in a friendly but formal tone.\n",
    "\n",
    "2. Legal return period\n",
    "State that customers have 14 days from receipt of goods to exercise their right of withdrawal (article L221-18 du Code de la consommation).\n",
    "\n",
    "3. Product condition \n",
    "Specify that returned items must be new, unwashed, unworn, with tags and in original packaging.\n",
    "\n",
    "4. Return procedure \n",
    "Explain clearly that the customer must:\n",
    "- Include the original delivery note inside the parcel.\n",
    "- Send it back to the address indicated on the label: 17 Avenue de l'Europe, 92270 Bois-Colombes.\n",
    "\n",
    "5. Refund and return costs\n",
    "Indicate that:\n",
    "- Return shipping is free only if an error or defect occurred.\n",
    "- Otherwise, it is at the customer's expense.\n",
    "- Refunds are processed within 10-14 business days upon receipt and inspection of goods.\n",
    "\n",
    "6. Final reassurance\n",
    "End the message by offering support for further questions and thanking the customer for their trust.\n",
    "The output must be a single, ready-to-send email formatted with correct paragraph spacing and without bullet points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeKAbcqDBgnk"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.requirement import req\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "# Add requirements to verify the generated email\n",
    "requirements_list = [\n",
    "    req(\"...\"),\n",
    "]\n",
    "\n",
    "full_prompt = (\n",
    "    email_prompt\n",
    "    + \"\"\"\n",
    "    \n",
    "    Client name:\n",
    "    {{client_name}} \n",
    "    \n",
    "    Client request:\n",
    "    {{client_request}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "email_candidate = m.instruct(\n",
    "    description=full_prompt,\n",
    "    requirements=requirements_list,\n",
    "    strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "    user_variables={\"client_name\": client_name, \"client_request\": client_request},\n",
    "    return_sampling_results=True,\n",
    ")\n",
    "\n",
    "print(email_candidate.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mellea library\n",
    "import mellea\n",
    "# Import additional mellea utils\n",
    "from mellea.stdlib.requirement import check, req, simple_validate\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "# Import display utils\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Start mella session with default model\n",
    "m = mellea.start_session()\n",
    "\n",
    "# Create a method to generate the email\n",
    "def generate_email(prompt, name, request):\n",
    "    # Add client information to the prompt\n",
    "    full_prompt = (\n",
    "        prompt\n",
    "        + \"\"\"\n",
    "        \n",
    "        Client name:\n",
    "        {{client_name}} \n",
    "        \n",
    "        Client request:\n",
    "        {{client_request}}\n",
    "        \"\"\"\n",
    "    )\n",
    "    # Add our requirements to check the generated email\n",
    "    requirements_list = [\n",
    "        req(\"The email should have a salutation\"),\n",
    "        req(\n",
    "            \"Include the website url: mellea.ai\",\n",
    "            validation_fn=simple_validate(lambda x: \"mellea.ai\" in x),\n",
    "        ),\n",
    "        check(\"Make sure the mail is in correct English\"),\n",
    "    ]\n",
    "    # Generate the email\n",
    "    email_candidate = m.instruct(\n",
    "        description=full_prompt,\n",
    "        requirements=requirements_list,\n",
    "        user_variables={\"client_name\": name, \"client_request\": request},\n",
    "        strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "        return_sampling_results=True,\n",
    "    )\n",
    "    # return the result, or the first generation if we don't reach a valid result\n",
    "    if email_candidate.success:\n",
    "        return email_candidate.value\n",
    "    else:\n",
    "        return email_candidate.sample_generations[0].value\n",
    "\n",
    "# Display the generated email\n",
    "display(Markdown(generate_email(email_prompt, client_name, client_request)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 3: Generative Stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eljw3pUP8WkP"
   },
   "source": [
    "## Mellea features\n",
    "\n",
    "In classical programming, pure (stateless) functions are a simple and powerful abstraction. A pure function takes inputs, computes outputs, and has no side effects. Generative programs can also use functions as abstraction boundaries, but in a generative program the meaning of the function can be given by an LLM instead of an interpreter or compiler. This is the idea behind a GenerativeSlot.\n",
    "\n",
    "A GenerativeSlot is a function whose implementation is provided by an LLM. In Mellea, you define these using the `@generative` decorator. The function signature specifies the interface, and the docstring (or type annotations) guide the LLM in producing the output. Let's start with a simple example of a sentiment classifier using the Generative interface.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Now, let's improve what we've build by setting up a classifier to analyze incoming customer emails at our apparel store by:\n",
    "-  detecting sentiment (positive, negative, or neutral)\n",
    "- identifying the specific reason for the return request (e.g., wrong size, poor fit, color mismatch, quality issues, style preference). \n",
    "\n",
    "Use these insights to automatically customize response templates, ensuring each reply addresses the customer's specific clothing concern with the appropriate tone and includes relevant information like size guides or exchange options.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `@generative` annotation to create a runnable method from a text description\n",
    "- determine possible outputs with the `Literal[]` type annnotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "@generative\n",
    "def sentiment_classifier(text: str) -> None | Literal[\"positive\", \"negative\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive or negative.\"\"\"\n",
    "\n",
    "\n",
    "sentiment = sentiment_classifier(m, text=\"The weather in Paris is beautiful today!\")\n",
    "print(f\"Detected sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Start mella session with default model\n",
    "m = mellea.start_session()\n",
    "\n",
    "# Generative function to detect the sentiment - can be positive, negative or neutral\n",
    "@generative\n",
    "def sentiment_classifier(\n",
    "    text: str,\n",
    ") -> None | Literal[\"positive\", \"negative\", \"neutral\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive, negative or neutral.\"\"\"\n",
    "\n",
    "\n",
    "# Generative function to detect the reason for return - can be size, color, or other\n",
    "@generative\n",
    "def return_reason_detection(text: str) -> None | Literal[\"size\", \"color\", \"other\"]:\n",
    "    \"\"\"Determine if the reason for the product return described in `text` is size or color, or other.\"\"\"\n",
    "\n",
    "\n",
    "# Detect email sentiment\n",
    "sentiment = sentiment_classifier(m, text=client_request)\n",
    "display(Markdown(f\"Detected sentiment: {sentiment}\"))\n",
    "\n",
    "# Detect reason for return\n",
    "reason = return_reason_detection(m, text=client_request)\n",
    "display(Markdown(f\"Detected reason: {reason}\"))\n",
    "\n",
    "# Adapt email prompt based on reason for return\n",
    "advanced_prompt = email_prompt\n",
    "if reason == \"size\":\n",
    "    advanced_prompt = (\n",
    "        advanced_prompt\n",
    "        + \"\"\"\n",
    "7. Additional information\n",
    "Mention the available sizes: XS, S, M, L, XL, XXL\n",
    "    \"\"\"\n",
    "    )\n",
    "elif reason == \"color\":\n",
    "    advanced_prompt = (\n",
    "        advanced_prompt\n",
    "        + \"\"\"\n",
    "7. Additional information\n",
    "Mention the available colors: Black, Blue, White, Yellow\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# Generate email with updated prompt\n",
    "display(Markdown(generate_email(advanced_prompt, client_name, client_request)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 4: Docling and Mellea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellea features\n",
    "\n",
    "In this lab, we will use both Docling and Mellea to extract data from a PDF.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Customers typically attach their purchase invoice when requesting a return. We'll automatically extract the product reference from these invoices using Docling. \n",
    "\n",
    "One of our customer just sent us a request with an invoice, let's try with this one. \n",
    "\n",
    "<center>\n",
    "<img src=\"./assets/Mellea%20Store%20-%20Invoice.png\" width=300 /><br/>\n",
    "<a href=\"https://raw.githubusercontent.com/remiserra/mellea-workshop/refs/heads/main/assets/Mellea%20Store%20-%20Invoice.pdf\" target=\"new\">Mellea Store - Invoice.pdf</a>\n",
    "</center>\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `Mellea Store - Invoice.pdf` file located at `asset_path`\n",
    "- use the `RichDocument.from_document_file` to load a pdf file\n",
    "- use `get_tables()` to retrieve the tables in the document\n",
    "- use `m.query()` to ask a question on the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVH1fmWB3JHp"
   },
   "outputs": [],
   "source": [
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "pdf = asset_path + \"Mellea%20Store%20-%20Invoice.pdf\"\n",
    "rd = RichDocument.from_document_file(pdf)\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mellea libraries\n",
    "import mellea\n",
    "# Import docling libraries\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load pdf document with docling\n",
    "pdf = asset_path + \"Mellea%20Store%20-%20Invoice.pdf\"\n",
    "rd = RichDocument.from_document_file(pdf)\n",
    "\n",
    "# Extract first table and display it\n",
    "table1: Table = rd.get_tables()[0]\n",
    "display(Markdown(table1.to_markdown()))\n",
    "\n",
    "# Start a mellea session to ask a question about the table\n",
    "m = mellea.start_session()\n",
    "ref = m.query(table1, \"what is the product number ?\").value\n",
    "display(Markdown(ref))\n",
    "\n",
    "# Parse to extract the product number\n",
    "display(Markdown(f'Product number: {ref.split(\"#\")[1]}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 5: Vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellea features\n",
    "\n",
    "Analyse an image by leveraging a vision model directly in Mellea <img src=\"./assets/mellea_pointing_up.jpg\" height=30 />\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "If the customer has lost their invoice, we can use a vision model to analyze a photo of the item they want to return. This allows us to identify and validate the product directly from the image, ensuring a smooth return process even without paperwork. The vision model should be capable of distinguishing between different items to prevent fraudulent returns.\n",
    "\n",
    "We can test the system using the following images:\n",
    "\n",
    "- The first picture shows an incorrect item, which the system should reject\n",
    "- The second picture with the black tee picture should be accepted as a valid return item\n",
    "\n",
    "\n",
    "[photo-mellea-shoe](./assets/photo-mellea-shoe.png)\n",
    "<img src=\"./assets/photo-mellea-shoe.png\" width=200 />\n",
    "[photo-mellea-tshirt](./assets/photo-mellea-tshirt.png)\n",
    "<img src=\"./assets/photo-mellea-tshirt.png\" width=200 />\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- start a mellea session with the `granite3.2-vision` vision model\n",
    "- load the `mellea_pointing_up.jpg` image from `asset_path`\n",
    "- pass the image as context to the `instruct()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "import requests\n",
    "from PIL import Image as PILImage\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# Initiate a session with a vision model\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# load image\n",
    "img_url = asset_path + \"mellea_pointing_up.jpg\"\n",
    "response = requests.get(img_url)\n",
    "test_img = PILImage.open(BytesIO(response.content))\n",
    "\n",
    "# ask a question about the image\n",
    "res = m.instruct(\"Is the subject in the image smiling?\", images=[test_img])\n",
    "\n",
    "display(Image(img_url, width=200),Markdown(f\"Result:{res}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "# Import requests, io and PIL libraries to handle remote images\n",
    "import requests\n",
    "from PIL import Image as PILImage\n",
    "from io import BytesIO\n",
    "# Import display utils\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# Initiate a session with a model vision\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# Method to ask a question on a image\n",
    "def ask_img(q, img_file):\n",
    "    # Build full image url \n",
    "    img_url = asset_path + img_file\n",
    "    # Get the remote image file\n",
    "    response = requests.get(img_url)\n",
    "    # Load the image as PIL image\n",
    "    test_img = PILImage.open(BytesIO(response.content))\n",
    "    # Ask a question, giving the image in the context, also require a Yes/No answer\n",
    "    res = m.instruct(q, images=[test_img], requirements=[req(\"Answer Yes or No\")])\n",
    "    # Display the image and answer\n",
    "    display(Image(img_url, width=200),Markdown(f\"{q}: {res.value}\"))\n",
    "\n",
    "# Analyse 1st image\n",
    "ask_img(\"Is this a T-shirt ?\", \"photo-mellea-shoe.png\")\n",
    "\n",
    "# Analyse 2nd image\n",
    "ask_img(\"Is this a T-shirt ?\", \"photo-mellea-tshirt.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you have completed the labs !\n",
    "\n",
    "<img src=\"./assets/mellea_pointing_up.jpg\" width=200 />\n",
    "\n",
    "## Additional resources:\n",
    "\n",
    "- https://mellea.ai\n",
    "- https://docs.mellea.ai\n",
    "- https://github.com/generative-computing/mellea\n",
    "- https://pypi.org/project/mellea/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "githubcom-rs-mellea-workshop (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
