{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRO_MFFj_iYh"
   },
   "source": [
    "<img src=\"./assets/mellea_banner.png\" width=\"80%\"/>\n",
    "\n",
    "# Mellea workshop\n",
    "\n",
    "[Notebook on github](https://github.com/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb),\n",
    "[Notebook on colab](https://colab.research.google.com/github/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb)\n",
    "\n",
    "This workshop will introduce the basics of Generative Computing through a series of labs following a use-case.\n",
    "\n",
    "During this workshop, we will:\n",
    "1. Get up an running with Mellea.\n",
    "2. See the Instruct - Validate - Repair pattern in action.\n",
    "3. Encapsulate LLM calls using a functional interface via Mellea's `@generative` decorator.\n",
    "4. Combine Mellea and Docling to write a generative program that operates over documents.\n",
    "5. Leverage a vision model to analyse an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "\n",
    "Run the first cell during our introduction. The first cell will:\n",
    " * download an install ollama on your Colab instance\n",
    " * pull the granite4 and granite-vision model weights from ollama\n",
    " * pull the docling model weights\n",
    "\n",
    "This should take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6fDEbLHL_hkK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install ollama\n",
      "Download model weights\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 6c02683809a8: 100% ▕██████████████████▏ 2.1 GB                         \u001b[K\n",
      "pulling 0f6ec9740c76: 100% ▕██████████████████▏ 7.1 KB                         \u001b[K\n",
      "pulling cfc7749b96f6: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling ba32b08db168: 100% ▕██████████████████▏  417 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 1aefcd9a8a15: 100% ▕██████████████████▏ 1.5 GB                         \u001b[K\n",
      "pulling 4d464be24899: 100% ▕██████████████████▏ 892 MB                         \u001b[K\n",
      "pulling 579046ba1157: 100% ▕██████████████████▏ 1.3 KB                         \u001b[K\n",
      "pulling 1fa69e2371b7: 100% ▕██████████████████▏  154 B                         \u001b[K\n",
      "pulling 2e68075caee4: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling e371ed40cc78: 100% ▕██████████████████▏   34 B                         \u001b[K\n",
      "pulling 2798fc1748ec: 100% ▕██████████████████▏  646 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Install Mellea with Docling\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m\n",
      "Run docling once to download model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:28,076 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-23 11:27:28,081 - INFO - Going to convert document batch...\n",
      "2025-10-23 11:27:28,082 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 77ade624737fd5e44f3478e1fd0b64f8\n",
      "2025-10-23 11:27:28,083 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-10-23 11:27:28,084 - INFO - Accelerator device: 'mps'\n",
      "2025-10-23 11:27:32,079 - INFO - Accelerator device: 'mps'\n",
      "2025-10-23 11:27:32,509 - INFO - Processing document 1906.04043v1.pdf\n",
      "2025-10-23 11:27:37,979 - INFO - Finished converting document 1906.04043v1.pdf in 10.18 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Install ollama\")\n",
    "# - When running locally, install ollama app from https://ollama.com, and the service should start automatically with mac/windows\n",
    "# - When running on colab uncomment the next two lines to install and start ollama in the colab environment:\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
    "# !nohup ollama serve >/dev/null 2>&1 &\n",
    "\n",
    "print(\"Download model weights\")\n",
    "# Download model weights for the Granite 4 and Granite vision models\n",
    "!ollama pull ibm/granite4:micro\n",
    "!ollama pull granite3.2-vision\n",
    "print(\"Install Mellea with Docling\")\n",
    "!uv pip install mellea docling jupyter ipywidgets \n",
    "print(\"Run docling once to download model weights\")\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")\n",
    "\n",
    "# Some UI niceness\n",
    "# from IPython.display import HTML, display  # noqa: E402\n",
    "\n",
    "# def set_css():\n",
    "#     display(HTML(\"\\n<style>\\n pre{\\n white-space: pre-wrap;\\n}\\n</style>\\n\"))\n",
    "\n",
    "# get_ipython().events.register(\"pre_run_cell\", set_css)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Mellea.ai\n",
    "\n",
    "## *“The second most remarkable thing about LLMs is that they can understand and produce fluent natural language. The most remarkable thing about them is that they do the wrong thing 5-50% of the time”*\n",
    "\n",
    "### > **We need structures** that make it a first-class concern **to check our work**, and ideally, we should build new tools to allow interrogation of certainty.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## What is “generative computing”?\n",
    "\n",
    "*Generative computing* is the idea that **LLMs** can function **as computing elements** that are a part of, not separate from, the rest of computer science.\n",
    "\n",
    "Like computing in general, this will have many facets, spanning engineering practice, system design, theory, hardware, software, etc.\n",
    "\n",
    "Many aspects are already emerging in the field, but they could use a nudge, and **we’re building tools to accelerate progress**.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Mellea.ai\n",
    "\n",
    "### Local-first Capability\n",
    "Generative programs can do Big Model Things without Big Model hardware.\n",
    "\n",
    "### Robust and Composable\n",
    "Requirement-driven inference pipelines result in rock-solid libraries and deployable apps.\n",
    "\n",
    "### Ready to Scale\n",
    "Swap out inference engines and models with ease. Massively scale inference in one line of code.\n",
    "\n",
    "### Open & Permissive\n",
    "The open inference stack for building generative programs on open weight models.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Applied Principles for Mellea.ai\n",
    "\n",
    "5 steps to showcase Mellea features with a unique workshop:\n",
    "\n",
    "### [01. Easy prompting](#lab-1-hello-mellea)\n",
    "Automated response via LLM\n",
    "\n",
    "### [02. Validate & Retry](#lab-2-instruct-validate-repair)\n",
    "Generated response guided-verification\n",
    "\n",
    "### [03. Function-Generation](#lab-3-generative-stubs)\n",
    "Satisfaction Detection through Stub Generation\n",
    "\n",
    "### [04. Embedded Document processing](#lab-4-docling-and-mellea)\n",
    "Invoice simplified Analysis\n",
    "\n",
    "### [05. Vision Model](#lab-5-vision-models)\n",
    "Product identification in a photo\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Use-case overview\n",
    "\n",
    "Let's sell T-Shirts on our [Mellea.ai Store](https://mellea-ai-store.figma.site/)!\n",
    "\n",
    "<iframe src=\"https://mellea-ai-store.figma.site/\" width=\"100%\" height=400 />\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpHGZpSt5mi4"
   },
   "source": [
    "# Lab 1: Hello, Mellea!\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "- Run locally with Ollama\n",
    "- Get started with 3 lines of code\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "<TBD>\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "Running `mellea.start_session()` initialize a new `MelleaSession`. The session holds three things:\n",
    "1. The model to use for this session. In this tutorial we will use granite3.3:8b.\n",
    "2. An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.\n",
    "3. A `Context`, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout this tutorial, we will be using a `SimpleContext`. In `SimpleContext`s, **every request starts with a fresh context**. There is no preserved chat history between requests. Mellea provides other types of context, but today we will not be using those features. See the Tutorial for further details.\n",
    "\n",
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MlF315Wa_pGh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:38,008 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:38,019 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:45,459 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM (International Business Machines Corporation) has been at the forefront of artificial intelligence (AI) since its inception in 1953.\n",
      "\n",
      "The term \"artificial intelligence\" was first coined by John McCarthy, an American computer scientist, when he attended a conference at Dartmouth College in 1956. IBM played a crucial role from the very beginning as it is one of the pioneers in this field.\n",
      "\n",
      "In 1959, IBM developed SNLA (Symbolic Natural Language Analysis), which was designed to perform natural language processing tasks. This marked IBM's first significant step into AI research and development.\n",
      "\n",
      "A major milestone came when Arthur Samuel, an IBM researcher, created a checkers-playing program that could learn from its own experiences. This demonstrated the potential of machine learning - a subset of AI where systems can analyze data and act upon it without being explicitly programmed to do so.\n",
      "\n",
      "In 1961, IBM developed the first prototype for what would become their renowned Expert System, called SAGE (Semi-Automatic Ground Environment). It was designed to help manage U.S. air defense against Soviet attacks during the Cold War period. While not strictly an 'AI' system in today's terms, it represented early attempts at using computers to mimic human decision-making processes.\n",
      "\n",
      "During the 1970s and 1980s, IBM continued investing heavily into AI research leading to advancements like expert systems (like MYCIN for medical diagnosis), natural language understanding software, machine translation tools, etc. \n",
      "\n",
      "In 1997, IBM's Deep Blue computer became a household name when it defeated world chess champion Garry Kasparov - an event that highlighted the potential of AI in complex strategic games.\n",
      "\n",
      "Today, IBM continues to lead in AI innovation with products like Watson (a question-answering system capable of understanding natural language queries) and various cloud-based platforms offering services for developing AI applications.\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "answer = m.chat(\"tell me about IBM and the early history of AI.\")\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "import mellea\n",
    "m = mellea.start_session()\n",
    "answer = m.chat(\"tell me about IBM and the early history of AI.\")\n",
    "print(answer.content)\n",
    "```\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI5ZHPOX7I-7"
   },
   "source": [
    "# Lab 2: Instruct-Validate-Repair\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:\n",
    "1. Instruct the model to perform a task and specify requirements on the output of the task.\n",
    "2. Validate that these requirements are satisfied by the model's output.\n",
    "3. If any requirements fail, try to repair.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "We will prepare an e-mail answer, and check for specific aspects in the generated email.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- Build a requirements list with the `req()` and `check()` constructors.\n",
    "- use the `instruct()` method to ask Mellea to perform an instruction and retry until the requirements are met.\n",
    "\n",
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EeKAbcqDBgnk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:45,502 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:45,507 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]2025-10-23 11:27:47,349 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:47,714 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:47,788 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:27:47-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:47,790 - INFO - SUCCESS\n",
      "  0%|          | 0/5 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: heartfelt appreciation for your invaluable assistance\n",
      "\n",
      "dear olivia,\n",
      "\n",
      "i hope this message finds you well. i am writing to express my sincere gratitude for the exceptional support and dedication you've shown in organizing events, advertising our speaker series, and resolving issues with snack delivery during the past few weeks.\n",
      "\n",
      "your efforts have not gone unnoticed and have significantly contributed to the success of our lab's initiatives. we are truly fortunate to have someone as committed and capable as you on board.\n",
      "\n",
      "thank you once again for your outstanding work and commitment. please accept this email as a token of our appreciation.\n",
      "\n",
      "warm regards,\n",
      "\n",
      "[your name]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.requirement import check, req, simple_validate\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "requirements_list = [\n",
    "    req(\"The email should have a salutation\"),\n",
    "    req(\n",
    "        \"Use only lower-case letters\",\n",
    "        validation_fn=simple_validate(lambda x: x.lower() == x),\n",
    "    ),\n",
    "    check(\"Do not mention purple elephants.\"),\n",
    "]\n",
    "\n",
    "name = \"Olivia\"\n",
    "notes = \"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\"\n",
    "\n",
    "email_candidate = m.instruct(\n",
    "    description=\"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
    "    requirements=requirements_list,\n",
    "    strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "    user_variables={\"name\": name, \"notes\": notes},\n",
    "    return_sampling_results=True,\n",
    ")\n",
    "if email_candidate.success:\n",
    "    print(email_candidate.result)\n",
    "else:\n",
    "    print(email_candidate.sample_generations[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "<TBD>\n",
    "```\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eljw3pUP8WkP"
   },
   "source": [
    "## Lab 3: Generative Stubs\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "In classical programming, pure (stateless) functions are a simple and powerful abstraction. A pure function takes inputs, computes outputs, and has no side effects. Generative programs can also use functions as abstraction boundaries, but in a generative program the meaning of the function can be given by an LLM instead of an interpreter or compiler. This is the idea behind a GenerativeSlot.\n",
    "\n",
    "A GenerativeSlot is a function whose implementation is provided by an LLM. In Mellea, you define these using the `@generative` decorator. The function signature specifies the interface, and the docstring (or type annotations) guide the LLM in producing the output. Let's start with a simple example of a sentiment classifier using the Generative interface.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Setup a classifier to detect the sentiment of the client's email, and use it to customize our answer.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `@generative` annotation to create a runnable method from a text description\n",
    "- determine possible outputs with the `Literal[]` type annnotation\n",
    "\n",
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:47,827 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:47,830 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]2025-10-23 11:27:48,199 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:27:48-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:48,203 - INFO - SUCCESS\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected sentiment: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]2025-10-23 11:27:48,552 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:27:48-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:48,555 - INFO - SUCCESS\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected reason: size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "@generative\n",
    "def sentiment_classifier(text: str) -> None | Literal[\"positive\", \"negative\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive or negative.\"\"\"\n",
    "\n",
    "@generative\n",
    "def return_reason_detection(text: str) -> None | Literal[\"size\", \"color\"]:\n",
    "    \"\"\"Determine if the reason for the product return described in `text` is size or color.\"\"\"\n",
    "\n",
    "sentiment = sentiment_classifier(m, text=\"The weather in Orlando is beautiful today!\")\n",
    "print(f\"Detected sentiment: {sentiment}\")\n",
    "\n",
    "reason = return_reason_detection(m, text=\"The Tshirt is too small.\")\n",
    "print(f\"Detected reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "@generative\n",
    "def sentiment_classifier(text: str) -> None | Literal[\"positive\", \"negative\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive or negative.\"\"\"\n",
    "\n",
    "@generative\n",
    "def return_reason_detection(text: str) -> None | Literal[\"size\", \"color\"]:\n",
    "    \"\"\"Determine if the reason for the product return described in `text` is size or color.\"\"\"\n",
    "\n",
    "sentiment = sentiment_classifier(m, text=\"The weather in Orlando is beautiful today!\")\n",
    "print(f\"Detected sentiment: {sentiment}\")\n",
    "\n",
    "reason = return_reason_detection(m, text=\"The Tshirt is too small.\")\n",
    "print(f\"Detected reason: {reason}\")\n",
    "```\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Docling and Mellea\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "In this lab, we will use both Docling and Mellea to extract data from a PDF.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Use docling to read the invoice attached by the client and check for the product reference.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `RichDocument.from_document_file` to load a pdf file\n",
    "- use `get_tables()` to retrieve the tables in the document\n",
    "- use `m.query()` to ask a question on the table\n",
    "\n",
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CVH1fmWB3JHp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:48,571 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-23 11:27:48,573 - INFO - Going to convert document batch...\n",
      "2025-10-23 11:27:48,573 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 77ade624737fd5e44f3478e1fd0b64f8\n",
      "2025-10-23 11:27:48,573 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-10-23 11:27:48,574 - INFO - Accelerator device: 'mps'\n",
      "2025-10-23 11:27:49,425 - INFO - Accelerator device: 'mps'\n",
      "2025-10-23 11:27:49,690 - INFO - Processing document Mellea Store - Invoice.pdf\n",
      "2025-10-23 11:27:50,885 - INFO - Finished converting document Mellea Store - Invoice.pdf in 2.32 sec.\n",
      "2025-10-23 11:27:50,907 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:50,919 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Item                           | Product Number   | Quantit y   | Unit Price   | Total   |\n",
      "|--------------------------------|------------------|-------------|--------------|---------|\n",
      "| Amanita Muscaria Tee (Black M) | #AMT3569         | 1           | $58          | $58     |\n",
      "|                                |                  |             | Subtotal     | $58     |\n",
      "|                                |                  |             | Tax (10%)    | $5.8    |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:51,737 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product number for Amanita Muscaria Tee (Black M) is #AMT3569.\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "rd = RichDocument.from_document_file(\"./assets/Mellea Store - Invoice.pdf\")\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())\n",
    "\n",
    "m = mellea.start_session()\n",
    "ref = m.query(table1, \"what is the product number ?\")\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "import mellea\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "rd = RichDocument.from_document_file(\"./assets/Mellea Store - Invoice.pdf\")\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())\n",
    "\n",
    "m = mellea.start_session()\n",
    "ref = m.query(table1,\"what is the product number ?\")\n",
    "print(ref)\n",
    "print(f'Product number: {ref.value.split(\"#\")[1]}')\n",
    "```\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Vision models\n",
    "\n",
    "## Mellea features\n",
    "\n",
    "Analyse an image by leveraging a vision model directly in Mellea\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Use a vision model to read a photo provided by the client and validate the item to be returned.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- start a mellea session with a model vision\n",
    "- load an image\n",
    "- pass the image as context to the `instruct()` method\n",
    "\n",
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:27:51,764 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/ps \"HTTP/1.1 200 OK\"\n",
      "2025-10-23 11:27:51,767 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]2025-10-23 11:28:06,593 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:28:06-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:28:06,600 - INFO - SUCCESS\n",
      "  0%|          | 0/2 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photo 1 result: \n",
      "no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]2025-10-23 11:28:12,179 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m=== 11:28:12-INFO ======\n",
      "SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 11:28:12,180 - INFO - SUCCESS\n",
      "  0%|          | 0/2 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photo 2 result: \n",
      "Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mellea\n",
    "from PIL import Image\n",
    "\n",
    "# Initiate a session with a model vision\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# load image\n",
    "test_img = Image.open(\"./assets/photo-mellea-shoe.png\")\n",
    "\n",
    "# ask a question about the 1st image\n",
    "res = m.instruct(\"Is this a T-shirt ?\", images=[test_img])\n",
    "print(f\"Photo 1 result: {res.value}\")\n",
    "\n",
    "# ask a question about the 2nd image\n",
    "print(\n",
    "    f\"Photo 2 result: {m.instruct(\"Is this a T-shirt ?\", images=[Image.open(\"./assets/photo-mellea-tshirt.png\")])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "```py\n",
    "import mellea\n",
    "from PIL import Image\n",
    "\n",
    "# Initiate a session with a model vision\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# load image\n",
    "test_img = Image.open(\"./assets/photo-mellea-shoe.png\")\n",
    "\n",
    "# ask a question about the 1st image\n",
    "res = m.instruct(\"Is this a T-shirt ?\", images=[test_img])\n",
    "print(f\"Photo 1 result: {res.value}\")\n",
    "\n",
    "# ask a question about the 2nd image\n",
    "print(\n",
    "    f\"Photo 2 result: {m.instruct(\"Is this a T-shirt ?\", images=[Image.open(\"./assets/photo-mellea-tshirt.png\")])}\"\n",
    ")\n",
    "```\n",
    "\n",
    "<hr/>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "githubcom-rs-mellea-workshop (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
