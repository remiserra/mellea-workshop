{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRO_MFFj_iYh"
   },
   "source": [
    "<img src=\"./assets/mellea_banner.png\" width=\"80%\"/>\n",
    "\n",
    "# Mellea workshop - [ibm.biz/mellea-workshop](https://ibm.biz/mellea-workshop)\n",
    "\n",
    "[Notebook on github](https://github.com/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb),\n",
    "[Notebook on colab](https://colab.research.google.com/github/remiserra/mellea-workshop/blob/main/mellea_workshop.ipynb)\n",
    "\n",
    "This workshop will introduce the basics of Generative Computing through a series of labs following a use-case.\n",
    "\n",
    "During this workshop, we will:\n",
    "1. Get up an running with Mellea.\n",
    "2. See the Instruct - Validate - Repair pattern in action.\n",
    "3. Encapsulate LLM calls using a functional interface via Mellea's `@generative` decorator.\n",
    "4. Combine Mellea and Docling to write a generative program that operates over documents.\n",
    "5. Leverage a vision model to analyse an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup\n",
    "\n",
    "Run the first cell during our introduction. The first cell will:\n",
    " * download an install ollama on your Colab instance\n",
    " * pull the granite4 and granite-vision model weights from ollama\n",
    " * pull the docling model weights\n",
    "\n",
    "This should take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fDEbLHL_hkK"
   },
   "outputs": [],
   "source": [
    "print(\"Install ollama\")\n",
    "# - When running locally, install ollama app from https://ollama.com, and the service should start automatically with mac/windows\n",
    "# - When running on colab uncomment the next two lines to install and start ollama in the colab environment:\n",
    "import subprocess\n",
    "result = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"Ollama already installed\")\n",
    "else:\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
    "    !nohup ollama serve >/dev/null 2>&1 &\n",
    "\n",
    "print(\"Download model weights\")\n",
    "# Download model weights for the Granite 4 and Granite vision models\n",
    "!ollama pull ibm/granite4:micro\n",
    "!ollama pull granite3.2-vision\n",
    "print(\"Install Mellea with Docling\")\n",
    "!uv pip install mellea docling jupyter ipywidgets \n",
    "print(\"Run docling once to download model weights\")\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")\n",
    "\n",
    "# Some UI niceness\n",
    "# from IPython.display import HTML, display  # noqa: E402\n",
    "\n",
    "# def set_css():\n",
    "#     display(HTML(\"\\n<style>\\n pre{\\n white-space: pre-wrap;\\n}\\n</style>\\n\"))\n",
    "\n",
    "# get_ipython().events.register(\"pre_run_cell\", set_css)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Mellea.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *“The second most remarkable thing about LLMs is that they can understand and produce fluent natural language. The most remarkable thing about them is that they do the wrong thing 5-50% of the time”*\n",
    "\n",
    "### > **We need structures** that make it a first-class concern **to check our work**, and ideally, we should build new tools to allow interrogation of certainty.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## What is “generative computing”?\n",
    "\n",
    "*Generative computing* is the idea that **LLMs** can function **as computing elements** that are a part of, not separate from, the rest of computer science.\n",
    "\n",
    "Like computing in general, this will have many facets, spanning engineering practice, system design, theory, hardware, software, etc.\n",
    "\n",
    "Many aspects are already emerging in the field, but they could use a nudge, and **we’re building tools to accelerate progress**.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## Mellea.ai\n",
    "\n",
    "### Local-first Capability\n",
    "Generative programs can do Big Model Things without Big Model hardware.\n",
    "\n",
    "### Robust and Composable\n",
    "Requirement-driven inference pipelines result in rock-solid libraries and deployable apps.\n",
    "\n",
    "### Ready to Scale\n",
    "Swap out inference engines and models with ease. Massively scale inference in one line of code.\n",
    "\n",
    "### Open & Permissive\n",
    "The open inference stack for building generative programs on open weight models.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Applied Principles for Mellea.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 steps to showcase Mellea features with a unique workshop:\n",
    "\n",
    "### 01. Easy prompting\n",
    "Automated response via LLM\n",
    "\n",
    "### 02. Validate & Retry\n",
    "Generated response guided-verification\n",
    "\n",
    "### 03. Function-Generation\n",
    "Generative functions through desciptions\n",
    "\n",
    "### 04. Embedded Document processing\n",
    "Easy integration of document processing pipeline\n",
    "\n",
    "### 05. Vision Model\n",
    "Analyse images with vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Use-case overview\n",
    "\n",
    "Let's sell T-Shirts on our [Mellea.ai Store](https://mellea-ai-store.figma.site/)!\n",
    "\n",
    "<iframe src=\"https://mellea-ai-store.figma.site/\" width=\"100%\" height=400></iframe>\n",
    "\n",
    "Our objective it to facilitate returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpHGZpSt5mi4"
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 1: Hello, Mellea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellea features\n",
    "\n",
    "- Run locally with Ollama\n",
    "- Get started with 3 lines of code\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "We will prepare the welcome message for our apparel retail website.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "Running `mellea.start_session()` initialize a new `MelleaSession`. The session holds three things:\n",
    "1. The model to use for this session. In this tutorial we will use granite3.3:8b.\n",
    "2. An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.\n",
    "3. A `Context`, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout this tutorial, we will be using a `SimpleContext`. In `SimpleContext`s, **every request starts with a fresh context**. There is no preserved chat history between requests. Mellea provides other types of context, but today we will not be using those features. See the Tutorial for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlF315Wa_pGh"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "answer = m.chat(\"Please write a welcome message for an apparel website\")\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 2: Instruct-Validate-Retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI5ZHPOX7I-7"
   },
   "source": [
    "## Mellea features\n",
    "\n",
    "Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:\n",
    "1. Instruct the model to perform a task and specify requirements on the output of the task.\n",
    "2. Validate that these requirements are satisfied by the model's output.\n",
    "3. If any requirements fail, try to repair.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "We will prepare an e-mail answer to a return request, and check for specific aspects in the generated email.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- Build a requirements list with the `req()` and `check()` constructors.\n",
    "- use the `instruct()` method to ask Mellea to perform an instruction and retry until the requirements are met.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the client name and request as inputs\n",
    "client_name = \"Joseph\"\n",
    "client_request = \"Hi, I just received my Amanita T-sirt, it's really nice ! Unforunately it is too small for me, how can I return it ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have instructions about the information to include in our emails\n",
    "email_prompt = \"\"\"\n",
    "You are an automated customer service representative for an apparel boutique called Mellea Store. Your task is to generate a formal, polite, and email-ready response to customers who contact the shop regarding a product return. \n",
    "\n",
    "Follow these rules when writing your answer:\n",
    "- Always reply using professional and courteous language suitable for an email. \n",
    "- Address the customer respectfully using “Hello” and close with “Regards” followed by “Mellea Store Team” \n",
    "- Include the standard French return policy terms compliant with consumer law. \n",
    "- The answer is concise. The lenght of the answer should be 120 words maximum.  \n",
    "\n",
    "Your email must contain the following sections: \n",
    "\n",
    "1. Acknowledgement of the request\n",
    "Confirm receipt of the return request in a friendly but formal tone.\n",
    "\n",
    "2. Legal return period\n",
    "State that customers have 14 days from receipt of goods to exercise their right of withdrawal (article L221-18 du Code de la consommation).\n",
    "\n",
    "3. Product condition \n",
    "Specify that returned items must be new, unwashed, unworn, with tags and in original packaging.\n",
    "\n",
    "4. Return procedure \n",
    "Explain clearly that the customer must:\n",
    "- Include the original delivery note inside the parcel.\n",
    "- Send it back to the address indicated on the label: 17 Avenue de l'Europe, 92270 Bois-Colombes.\n",
    "\n",
    "5. Refund and return costs\n",
    "Indicate that:\n",
    "- Return shipping is free only if an error or defect occurred.\n",
    "- Otherwise, it is at the customer's expense.\n",
    "- Refunds are processed within 10-14 business days upon receipt and inspection of goods.\n",
    "\n",
    "6. Final reassurance\n",
    "End the message by offering support for further questions and thanking the customer for their trust.\n",
    "The output must be a single, ready-to-send email formatted with correct paragraph spacing and without bullet points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeKAbcqDBgnk"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.requirement import req\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "# Add requirements to verify the generated email\n",
    "requirements_list = [\n",
    "    req(\"...\"),\n",
    "]\n",
    "\n",
    "full_prompt = (\n",
    "    email_prompt\n",
    "    + \"\"\"\n",
    "    \n",
    "    Client name:\n",
    "    {{client_name}} \n",
    "    \n",
    "    Client request:\n",
    "    {{client_request}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "email_candidate = m.instruct(\n",
    "    description=full_prompt,\n",
    "    requirements=requirements_list,\n",
    "    strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "    user_variables={\"client_name\": client_name, \"client_request\": client_request},\n",
    "    return_sampling_results=True,\n",
    ")\n",
    "\n",
    "print(email_candidate.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.requirement import check, req, simple_validate\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "def generate_email(prompt, name, request):\n",
    "    full_prompt = (\n",
    "        prompt\n",
    "        + \"\"\"\n",
    "        \n",
    "        Client name:\n",
    "        {{client_name}} \n",
    "        \n",
    "        Client request:\n",
    "        {{client_request}}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    requirements_list = [\n",
    "        req(\"The email should have a salutation\"),\n",
    "        req(\n",
    "            \"Include the website url: mellea.ai\",\n",
    "            validation_fn=simple_validate(lambda x: \"mellea.ai\" in x),\n",
    "        ),\n",
    "        check(\"Make sure the mail is in correct English\"),\n",
    "    ]\n",
    "\n",
    "    email_candidate = m.instruct(\n",
    "        description=full_prompt,\n",
    "        requirements=requirements_list,\n",
    "        strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "        user_variables={\"client_name\": name, \"client_request\": request},\n",
    "        return_sampling_results=True,\n",
    "    )\n",
    "    if email_candidate.success:\n",
    "        return email_candidate.result\n",
    "    else:\n",
    "        return email_candidate.sample_generations[0].value\n",
    "\n",
    "\n",
    "print(generate_email(email_prompt, client_name, client_request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 3: Generative Stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eljw3pUP8WkP"
   },
   "source": [
    "## Mellea features\n",
    "\n",
    "In classical programming, pure (stateless) functions are a simple and powerful abstraction. A pure function takes inputs, computes outputs, and has no side effects. Generative programs can also use functions as abstraction boundaries, but in a generative program the meaning of the function can be given by an LLM instead of an interpreter or compiler. This is the idea behind a GenerativeSlot.\n",
    "\n",
    "A GenerativeSlot is a function whose implementation is provided by an LLM. In Mellea, you define these using the `@generative` decorator. The function signature specifies the interface, and the docstring (or type annotations) guide the LLM in producing the output. Let's start with a simple example of a sentiment classifier using the Generative interface.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Setup a classifier to \n",
    "- detect the sentiment of the client's email\n",
    "- detect the reason for return \n",
    "and use this information to customize our answer\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `@generative` annotation to create a runnable method from a text description\n",
    "- determine possible outputs with the `Literal[]` type annnotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "\n",
    "@generative\n",
    "def sentiment_classifier(text: str) -> None | Literal[\"positive\", \"negative\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive or negative.\"\"\"\n",
    "\n",
    "\n",
    "sentiment = sentiment_classifier(m, text=\"The weather in Orlando is beautiful today!\")\n",
    "print(f\"Detected sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea import generative\n",
    "from typing import Literal\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "\n",
    "@generative\n",
    "def sentiment_classifier(text: str) -> None | Literal[\"positive\", \"negative\"]:\n",
    "    \"\"\"Determine if the sentiment of `text` is positive or negative.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def return_reason_detection(text: str) -> None | Literal[\"size\", \"color\"]:\n",
    "    \"\"\"Determine if the reason for the product return described in `text` is size or color.\"\"\"\n",
    "\n",
    "\n",
    "sentiment = sentiment_classifier(m, text=client_request)\n",
    "print(f\"Detected sentiment: {sentiment}\")\n",
    "\n",
    "reason = return_reason_detection(m, text=client_request)\n",
    "print(f\"Detected reason: {reason}\")\n",
    "\n",
    "advanced_prompt = email_prompt\n",
    "\n",
    "if reason == \"size\":\n",
    "    advanced_prompt = (\n",
    "        advanced_prompt\n",
    "        + \"\"\"\n",
    "7. Additional information\n",
    "Mention the available sizes: XS, S, M, L, XL, XXL\n",
    "    \"\"\"\n",
    "    )\n",
    "elif reason == \"color\":\n",
    "    advanced_prompt = (\n",
    "        advanced_prompt\n",
    "        + \"\"\"\n",
    "7. Additional information\n",
    "Mention the available colors: Black, Blue, White, Yellow\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "print(advanced_prompt)\n",
    "\n",
    "print(generate_email(advanced_prompt, client_name, client_request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 4: Docling and Mellea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellea features\n",
    "\n",
    "In this lab, we will use both Docling and Mellea to extract data from a PDF.\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "Use docling to read the invoice attached by the client and check for the product reference.\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- use the `RichDocument.from_document_file` to load a pdf file\n",
    "- use `get_tables()` to retrieve the tables in the document\n",
    "- use `m.query()` to ask a question on the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVH1fmWB3JHp"
   },
   "outputs": [],
   "source": [
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "rd = RichDocument.from_document_file(\"./assets/Mellea Store - Invoice.pdf\")\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "rd = RichDocument.from_document_file(\"./assets/Mellea Store - Invoice.pdf\")\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())\n",
    "\n",
    "m = mellea.start_session()\n",
    "ref = m.query(table1, \"what is the product number ?\")\n",
    "print(ref)\n",
    "\n",
    "print(f'Product number: {ref.value.split(\"#\")[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Lab 5: Vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mellea features\n",
    "\n",
    "Analyse an image by leveraging a vision model directly in Mellea <img src=\"./assets/mellea_pointing_up.jpg\" height=30 />\n",
    "\n",
    "## Use-case illustration\n",
    "\n",
    "If the client has lost the invoce, we can use a vision model to read a photo provided by the client and validate the item to be returned.\n",
    "\n",
    "We can use the following images:\n",
    "\n",
    "[photo-mellea-shoe](./assets/photo-mellea-shoe.png)\n",
    "<img src=\"./assets/photo-mellea-shoe.png\" width=200 />\n",
    "[photo-mellea-tshirt](./assets/photo-mellea-tshirt.png)\n",
    "<img src=\"./assets/photo-mellea-tshirt.png\" width=200 />\n",
    "\n",
    "## Lab guidelines\n",
    "\n",
    "- start a mellea session with a model vision\n",
    "- load an image\n",
    "- pass the image as context to the `instruct()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from PIL import Image\n",
    "\n",
    "# Initiate a session with a model vision\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# load image\n",
    "test_img = Image.open(\"./assets/mellea_pointing_up.jpg\")\n",
    "\n",
    "# ask a question about the image\n",
    "res = m.instruct(\"Is the subject in the image smiling?\", images=[test_img])\n",
    "print(f\"Result:{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mellea\n",
    "from PIL import Image\n",
    "\n",
    "# Initiate a session with a model vision\n",
    "m = mellea.start_session(model_id=\"granite3.2-vision\")\n",
    "\n",
    "# load image\n",
    "test_img = Image.open(\"./assets/photo-mellea-shoe.png\")\n",
    "\n",
    "# ask a question about the 1st image\n",
    "res = m.instruct(\"Is this a T-shirt ?\", images=[test_img])\n",
    "print(f\"Photo 1 result: {res.value}\")\n",
    "\n",
    "# ask a question about the 2nd image\n",
    "print(\n",
    "    f\"Photo 2 result: {m.instruct(\"Is this a T-shirt ?\", images=[Image.open(\"./assets/photo-mellea-tshirt.png\")])}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "githubcom-rs-mellea-workshop (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
