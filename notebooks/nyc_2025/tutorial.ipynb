{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRO_MFFj_iYh"
   },
   "source": [
    "# NYC Engineering Excellence Day 2025 Tutorial\n",
    "\n",
    "This tutorial will introduce the basics of Generative Computing through a series of labs. Everything you need is in this Notebook.\n",
    "\n",
    "During this tutorial, we will:\n",
    "1. Get up an running with Mellea.\n",
    "2. See the Instruct - Validate - Repair pattern in action.\n",
    "3. Encapsulate LLM calls using a functional interface via Mellea's `@generative` decorator.\n",
    "4. Combine Mellea and Docling to write a generative program that operates over documents.\n",
    "5. Encapsulate LLM calls using an object-oriented interface via Mellea's `MObject` protocol.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Run the first cell during our introduction. The first cell will:\n",
    " * download an install ollama on your Colab instance\n",
    " * download the `granite3.3:8b` model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fDEbLHL_hkK"
   },
   "outputs": [],
   "source": [
    "# Install ollama.\n",
    "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
    "!nohup ollama serve >/dev/null 2>&1 &\n",
    "\n",
    "# Download the granite:3.3:8b weights.\n",
    "!ollama pull granite3.3:8b\n",
    "!ollama pull llama3.2:3b\n",
    "\n",
    "# install Mellea.\n",
    "!uv pip install mellea[all] -q\n",
    "\n",
    "# Run docling once to download model weights.\n",
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "\n",
    "RichDocument.from_document_file(\"https://mellea.ai\")\n",
    "\n",
    "# Some UI niceness.\n",
    "from IPython.display import HTML, display  # noqa: E402\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    display(HTML(\"\\n<style>\\n pre{\\n white-space: pre-wrap;\\n}\\n</style>\\n\"))\n",
    "\n",
    "\n",
    "get_ipython().events.register(\"pre_run_cell\", set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpHGZpSt5mi4"
   },
   "source": [
    "# Lab 1: Hello, Mellea!\n",
    "\n",
    "Running `mellea.start_session()` initialize a new `MelleaSession`. The session holds three things:\n",
    "1. The model to use for this session. In this tutorial we will use granite3.3:8b.\n",
    "2. An inference engine; i.e., the code that actually calls our model. We will be using ollama, but you can also use Huggingface or any OpenAI-compatible endpoint.\n",
    "3. A `Context`, which tells Mellea how to remember context between requests. This is sometimes called the \"Message History\" in other frameworks. Throughout this tutorial, we will be using a `SimpleContext`. In `SimpleContext`s, **every request starts with a fresh context**. There is no preserved chat history between requests. Mellea provides other types of context, but today we will not be using those features. See the Tutorial for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlF315Wa_pGh"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "\n",
    "m = mellea.start_session()\n",
    "\n",
    "answer = m.chat(\n",
    "    \"tell me some fun trivial about IBM and the early history of AI.\"\n",
    ")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI5ZHPOX7I-7"
   },
   "source": [
    "# Lab 2: Instruct-Validate-Repair\n",
    "\n",
    "Instruct-Validate-Repair is a design pattern for building robust automation using LLMs. The idea is simple:\n",
    "1. Instruct the model to perform a task and specify requirements on the output of the task.\n",
    "2. Validate that these requirements are satisfied by the model's output.\n",
    "3. If any requirements fail, try to repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeKAbcqDBgnk"
   },
   "outputs": [],
   "source": [
    "import mellea\n",
    "from mellea.stdlib.requirement import check, req, simple_validate\n",
    "from mellea.stdlib.sampling import RejectionSamplingStrategy\n",
    "\n",
    "requirements = [\n",
    "    req(\"The email should have a salutation\"),  # == r1\n",
    "    req(\n",
    "        \"Use only lower-case letters\",\n",
    "        validation_fn=simple_validate(lambda x: x.lower() == x),\n",
    "    ),  # == r2\n",
    "    check(\"Do not mention purple elephants.\"),  # == r3\n",
    "]\n",
    "\n",
    "\n",
    "def write_email(m: mellea.MelleaSession, name: str, notes: str) -> str:\n",
    "    email_candidate = m.instruct(\n",
    "        \"Write an email to {{name}} using the notes following: {{notes}}.\",\n",
    "        requirements=requirements,\n",
    "        strategy=RejectionSamplingStrategy(loop_budget=5),\n",
    "        user_variables={\"name\": name, \"notes\": notes},\n",
    "        return_sampling_results=True,\n",
    "    )\n",
    "    if email_candidate.success:\n",
    "        return str(email_candidate.result)\n",
    "    else:\n",
    "        return email_candidate.sample_generations[0].value\n",
    "\n",
    "\n",
    "m = mellea.start_session()\n",
    "print(\n",
    "    write_email(\n",
    "        m,\n",
    "        \"Olivia\",\n",
    "        \"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eljw3pUP8WkP"
   },
   "source": [
    "## Lab 3: Writing Compositional Code with Generative Stubs\n",
    "\n",
    "In classical programming, pure (stateless) functions are a simple and powerful abstraction. A pure function takes inputs, computes outputs, and has no side effects. Generative programs can also use functions as abstraction boundaries, but in a generative program the meaning of the function can be given by an LLM instead of an interpreter or compiler. This is the idea behind a GenerativeSlot.\n",
    "\n",
    "A GenerativeSlot is a function whose implementation is provided by an LLM. In Mellea, you define these using the `@generative` decorator. The function signature specifies the interface, and the docstring (or type annotations) guide the LLM in producing the output.\n",
    "\n",
    "In this lab, we will see how **compositionality checks** can be used to combine libraries of generative functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSt2p92X1HFh"
   },
   "outputs": [],
   "source": [
    "from mellea import generative\n",
    "\n",
    "################################################################################\n",
    "# SUMMARIZER LIBRARY                                                           #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "@generative\n",
    "def summarize_meeting(transcript: str) -> str:\n",
    "    \"\"\"Summarize the meeting transcript into a concise paragraph of main points.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def summarize_contract(contract_text: str) -> str:\n",
    "    \"\"\"Produce a natural language summary of contract obligations and risks.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def summarize_short_story(story: str) -> str:\n",
    "    \"\"\"Summarize a short story, with one paragraph on plot and one paragraph on broad themes.\"\"\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# DECISION AIDES LIBRARY                                                       #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# The Decision Aides Library\n",
    "@generative\n",
    "def propose_business_decision(summary: str) -> str:\n",
    "    \"\"\"Given a structured summary with clear recommendations, propose a business decision.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def generate_risk_mitigation(summary: str) -> str:\n",
    "    \"\"\"If the summary contains risk elements, propose mitigation strategies.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def generate_novel_recommendations(summary: str) -> str:\n",
    "    \"\"\"Provide a list of novel recommendations that are similar in plot or theme to the short story summary.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u73dQjewCBFK"
   },
   "source": [
    "### Summarizing a meeting\n",
    "\n",
    "Let's use the meeting summarizer to summarize a meeting transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_ndEFA721cV"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Transcript of a meeting discussing risks                                     #\n",
    "################################################################################\n",
    "\n",
    "transcript = \"\"\"Meeting Transcript: Market Risk Review -- Self-Sealing Stembolts Division\n",
    "Date: December 1, 3125\n",
    "Attendees:\n",
    "\n",
    "Karen Rojas, VP of Product Strategy\n",
    "\n",
    "Derek Madsen, Director of Global Procurement\n",
    "\n",
    "Felicia Zheng, Head of Market Research\n",
    "\n",
    "Tom Vega, CFO\n",
    "\n",
    "Luis Tran, Engineering Liaison\n",
    "\n",
    "Karen Rojas:\n",
    "Thanks, everyone, for making time on short notice. As you've all seen, we've got three converging market risks we need to address: tariffs on micro-carburetors, increased adoption of the self-interlocking leafscrew, and, believe it or not, the \"hipsterfication\" of the construction industry. I need all on deck and let's not waste time. Derek, start.\n",
    "\n",
    "Derek Madsen:\n",
    "Right. As of Monday, the 25% tariff on micro-carburetors sourced from the Pan-Alpha Centauri confederacy is active. We tried to pre-purchase a three-month buffer, but after that, our unit cost rises by $1.72. That's a 9% increase in the BOM cost of our core model 440 stembolt. Unless we find alternative suppliers or pass on the cost, we're eating into our already narrow margin.\n",
    "\n",
    "Tom Vega:\n",
    "We cannot absorb that without consequences. If we pass the cost downstream, we risk losing key mid-tier OEM clients. And with the market already sniffing around leafscrew alternatives, this makes us more vulnerable.\n",
    "\n",
    "Karen:\n",
    "Lets pause there. Felicia, give us the quick-and-dirty on the leafscrew.\n",
    "\n",
    "Felicia Zheng:\n",
    "It's ugly. Sales of the self-interlocking leafscrew—particularly in modular and prefab construction—are up 38% year-over-year. It's not quite a full substitute for our self-sealing stembolts, but they are close enough in function that some contractors are making the switch. Their appeal? No micro-carburetors, lower unit complexity, and easier training for install crews. We estimate we've lost about 12% of our industrial segment to the switch in the last two quarters.\n",
    "\n",
    "Karen:\n",
    "Engineering, Luis; your take on how real that risk is?\n",
    "\n",
    "Luis Tran:\n",
    "Technically, leafscrews are not as robust under high-vibration loads. But here's the thing: most of the modular prefab sites don not need that level of tolerance. If the design spec calls for durability over 10 years, we win. But for projects looking to move fast and hit 5-year lifespans? The leafscrew wins on simplicity and cost.\n",
    "\n",
    "Tom:\n",
    "So they're eating into our low-end. That's our volume base.\n",
    "\n",
    "Karen:\n",
    "Exactly. Now let's talk about this last one: the “hipsterfication” of construction. Felicia?\n",
    "\n",
    "Felicia:\n",
    "So this is wild. We're seeing a cultural shift in boutique and residential construction—especially in markets like Beckley, West Sullivan, parts of Osborne County, where clients are requesting \"authentic\" manual fasteners. They want hand-sealed bolts, visible threads, even mismatched patinas. It's an aesthetic thing. Function is almost secondary. Our old manual-seal line from the 3180s? People are hunting them down on auction sites.\n",
    "\n",
    "Tom:\n",
    "Well, I'm glad I don't have to live in the big cities... nothing like this would ever happen in downt-to-earth places Brooklyn, Portland, or Austin.\n",
    "\n",
    "Luis:\n",
    "We literally got a request from a design-build firm in Keough asking if we had any bolts “pre-distressed.”\n",
    "\n",
    "Karen:\n",
    "Can we spin this?\n",
    "\n",
    "Tom:\n",
    "If we keep our vintage tooling and market it right, maybe. But that's niche. It won't offset losses in industrial and prefab.\n",
    "\n",
    "Karen:\n",
    "Not yet. But we may need to reframe it as a prestige line—low volume, high margin. Okay, action items. Derek, map alternative micro-carburetor sources. Felicia, get me a forecast on leafscrew erosion by sector. Luis, feasibility of reviving manual seal production. Tom, let's scenario-plan cost pass-through vs. feature-based differentiation.\n",
    "\n",
    "Let's reconvene next week with hard numbers. Thanks, all.\"\"\"\n",
    "\n",
    "summary = summarize_meeting(m, transcript=transcript)\n",
    "print(f\"Summary of meeting: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkyptU19CH9I"
   },
   "source": [
    "### Composing Summarizers with decision aides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zKo1rn016LX"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# COMPOSITIONALITY CHECKS                                                      #\n",
    "################################################################################\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "@generative\n",
    "def has_structured_conclusion(summary: str) -> Literal[\"yes\", \"no\"]:\n",
    "    \"\"\"Determine whether the summary contains a clearly marked conclusion or recommendation.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def contains_actionable_risks(summary: str) -> Literal[\"yes\", \"no\"]:\n",
    "    \"\"\"Check whether the summary contains references to business risks or exposure.\"\"\"\n",
    "\n",
    "\n",
    "@generative\n",
    "def has_theme_and_plot(summary: str) -> Literal[\"yes\", \"no\"]:\n",
    "    \"\"\"Check whether the summary contains both a plot and thematic elements.\"\"\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# APPLY DECISION AIDES                                                      #\n",
    "################################################################################\n",
    "\n",
    "# generate risk mitigation straegies based upon the meeting summary.\n",
    "if contains_actionable_risks(m, summary=summary) == \"yes\":\n",
    "    mitigation = generate_risk_mitigation(m, summary=summary)\n",
    "    print(f\"Mitigation: {mitigation}\")\n",
    "else:\n",
    "    print(\"Summary does not contain actionable risks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nZF6IUY2wFO"
   },
   "outputs": [],
   "source": [
    "if has_structured_conclusion(m, summary=summary) == \"yes\":\n",
    "    decision = propose_business_decision(m, summary=summary)\n",
    "    print(f\"Decision: {decision}\")\n",
    "else:\n",
    "    print(\"Summary lacks a structured conclusion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4: Docling and Mellea\n",
    "\n",
    "In this lab, we will use both Docling and Mellea to extract and then modify data in a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXDf7AiU3Bfk"
   },
   "outputs": [],
   "source": [
    "from mellea.stdlib.docs.richdocument import RichDocument\n",
    "\n",
    "rd = RichDocument.from_document_file(\"https://arxiv.org/pdf/1906.04043\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vtWw2mj3F4V"
   },
   "source": [
    "## Extract table from the document\n",
    "\n",
    "We can use docling to extract documents from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVH1fmWB3JHp"
   },
   "outputs": [],
   "source": [
    "from mellea.stdlib.docs.richdocument import Table\n",
    "\n",
    "table1: Table = rd.get_tables()[0]\n",
    "print(table1.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZupSeHZ3NoG"
   },
   "source": [
    "# Lab 4: Working with the Table Object\n",
    "\n",
    "The Table object is Mellea-ready and can be used immediately with LLMs. In this example, table1 is transformed to have an extra column \"Model\" which contains the model string from the Feature column or \"None\" if there is none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zucd8llC3Ri5"
   },
   "outputs": [],
   "source": [
    "from mellea.backends.model_ids import META_LLAMA_3_2_3B\n",
    "from mellea.backends.ollama import OllamaModelBackend\n",
    "from mellea.backends.types import ModelOption\n",
    "\n",
    "# You can use multiple different models at the same time!\n",
    "m_llama = mellea.MelleaSession(backend=OllamaModelBackend(model_id=META_LLAMA_3_2_3B))\n",
    "\n",
    "for seed in [x * 12 for x in range(5)]:\n",
    "    table2 = m_llama.transform(\n",
    "        table1,\n",
    "        \"Add a 'Model' column as the last column that extracts which model was used for that feature or 'None' if none.\",\n",
    "        model_options={ModelOption.SEED: seed},\n",
    "    )\n",
    "    if isinstance(table2, Table):\n",
    "        print(table2.to_markdown())\n",
    "    else:\n",
    "        print(\"==== TRYING AGAIN after non-useful output.====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RktHMyyZ5gBW"
   },
   "source": [
    "The model has fulfilled the task and coming back with a parsable syntax. You could now call (e.g. m.query(table2, \"Are there any GPT models referenced?\")) or continue transformation (e.g. m.transform(table2, \"Transpose the table.\")).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Generative Objects\n",
    "\n",
    "Object-oriented programming (OOP) is a powerful paradigm for organizing code: you group related data and the methods that operate on that data into classes. In the world of LLMs, a similar organizational principle emerges—especially when you want to combine structured data with LLM-powered \"tools\" or operations. This is where Mellea's MObject abstraction comes in.\n",
    "\n",
    "**The MObject Pattern**: You should store data alongside its relevant operations (tools). This allows LLMs to interact with both the data and methods in a unified, structured manner. It also simplifies the process of exposing only the specific fields and methods you want the LLM to access.\n",
    "\n",
    "The MObject pattern also provides a way of evolving existing classical codebases into generative programs. Mellea's @mify decorator lets you turn any class into an MObject. If needed, you can specify which fields and methods are included, and provide a template for how the object should be represented to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O87RWbRf5hSD"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from mellea.stdlib.mify import mify\n",
    "\n",
    "\n",
    "@mify(fields_include={\"table\"}, template=\"{{ table }}\")\n",
    "class MyCompanyDatabase:\n",
    "    table: str = \"\"\"| Store      | Sales   |\n",
    "| ---------- | ------- |\n",
    "| Northeast  | $250    |\n",
    "| Southeast  | $80     |\n",
    "| Midwest    | $420    |\"\"\"\n",
    "\n",
    "    def __init__(self, *, table: str | None = None):\n",
    "        if table is not None:\n",
    "            self.table = table\n",
    "\n",
    "    def _parse_table(self, table: str) -> pd.DataFrame:\n",
    "        # Clean up the markdown table\n",
    "        # Drop the separator row and strip whitespace/pipes\n",
    "        lines = [\n",
    "            line.strip()\n",
    "            for line in table.strip().splitlines()\n",
    "            if not set(line.strip()) <= {\"|\", \"-\", \" \"}\n",
    "        ]\n",
    "        cleaned = \"\\n\".join(lines)\n",
    "\n",
    "        # Read into dataframe\n",
    "        df = pd.read_csv(StringIO(cleaned), sep=\"|\")\n",
    "        df = df.rename(columns=lambda x: x.strip())  # strip spaces from column names\n",
    "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        return df\n",
    "\n",
    "    def update_sales(self, store: str, amount: str):\n",
    "        \"\"\"Update the sales for a specific store.\"\"\"\n",
    "        df = self._parse_table(self.table)\n",
    "        df.loc[df[\"Store\"] == store, \"Sales\"] = amount\n",
    "        return MyCompanyDatabase(table=df.to_csv(sep=\"|\", index=False, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9XGwqle7DsD"
   },
   "outputs": [],
   "source": [
    "db = MyCompanyDatabase()\n",
    "print(m.query(db, \"What were sales for the Northeast branch this month?\"))\n",
    "db = m.transform(db, \"Update the northeast sales to 1250.\")\n",
    "print(m.query(db, \"What were sales for the Northeast branch this month?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
